{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Llama 3 with QLoRA on Alpaca Dataset\n",
        "\n",
        "This notebook guides through the process of fine-tuning a Llama 3 model using QLoRA (Quantized Low-Rank Adaptation) on a alpaca dataset"
      ],
      "metadata": {
        "id": "q0hqKNzLsZ06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Environment Preparation\n",
        "\n",
        "### 1. Check GPU Availability\n",
        "\n",
        "First, let's make sure we have access to a GPU:"
      ],
      "metadata": {
        "id": "oOF2xv2SsZ1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:47:23.609816Z",
          "iopub.execute_input": "2025-05-17T20:47:23.609982Z",
          "iopub.status.idle": "2025-05-17T20:47:23.781075Z",
          "shell.execute_reply.started": "2025-05-17T20:47:23.609967Z",
          "shell.execute_reply": "2025-05-17T20:47:23.780177Z"
        },
        "id": "u42sAMvCsZ1C",
        "outputId": "3cf5f1da-6194-4ece-815c-4142de5ccfec"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Sat May 17 20:47:23 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Install Required Libraries"
      ],
      "metadata": {
        "id": "nDKMOa8vsZ1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers peft bitsandbytes accelerate trl datasets scipy tensorboard"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:47:23.782098Z",
          "iopub.execute_input": "2025-05-17T20:47:23.782403Z",
          "iopub.status.idle": "2025-05-17T20:48:43.426580Z",
          "shell.execute_reply.started": "2025-05-17T20:47:23.782378Z",
          "shell.execute_reply": "2025-05-17T20:48:43.425622Z"
        },
        "id": "EQJXJM3ksZ1G",
        "outputId": "6f7f59d9-45a4-4342-9ea3-17ffb813e749"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:48:43.428953Z",
          "iopub.execute_input": "2025-05-17T20:48:43.429420Z",
          "iopub.status.idle": "2025-05-17T20:48:46.434013Z",
          "shell.execute_reply.started": "2025-05-17T20:48:43.429393Z",
          "shell.execute_reply": "2025-05-17T20:48:46.432928Z"
        },
        "id": "um0IGucJsZ1H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Configure Hugging Face Access"
      ],
      "metadata": {
        "id": "ScpADisssZ1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "login()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:48:46.435127Z",
          "iopub.execute_input": "2025-05-17T20:48:46.435433Z",
          "iopub.status.idle": "2025-05-17T20:48:46.809656Z",
          "shell.execute_reply.started": "2025-05-17T20:48:46.435400Z",
          "shell.execute_reply": "2025-05-17T20:48:46.808889Z"
        },
        "colab": {
          "referenced_widgets": [
            "a1fcda15b8104992b8982f9489c86f29"
          ]
        },
        "id": "c-fhfen3sZ1H",
        "outputId": "ae23ce59-1fdc-456b-b8e3-ec5d41da48e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1fcda15b8104992b8982f9489c86f29"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation and Exploration\n",
        "\n",
        "### 1. Load and Explore the Dataset"
      ],
      "metadata": {
        "id": "OW4Kly6PsZ1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def seed_everything(seed):\n",
        "    \"\"\"Set seed for all random number generators.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "SEED = 123\n",
        "seed_everything(SEED)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:49:55.107222Z",
          "iopub.execute_input": "2025-05-17T20:49:55.107796Z",
          "iopub.status.idle": "2025-05-17T20:49:55.115025Z",
          "shell.execute_reply.started": "2025-05-17T20:49:55.107773Z",
          "shell.execute_reply": "2025-05-17T20:49:55.114410Z"
        },
        "id": "9RmPWeQQsZ1I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(\"Sample entry:\", dataset[6])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:49:55.126901Z",
          "iopub.execute_input": "2025-05-17T20:49:55.127369Z",
          "iopub.status.idle": "2025-05-17T20:49:56.949928Z",
          "shell.execute_reply.started": "2025-05-17T20:49:55.127347Z",
          "shell.execute_reply": "2025-05-17T20:49:56.949110Z"
        },
        "id": "DusZRUILsZ1J",
        "outputId": "231e9858-bb56-45f6-f6d9-55270d10fa3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Dataset size: 51760\nSample entry: {'output': 'The fraction 4/16 is equivalent to 1/4 because both fractions represent the same value. A fraction can be simplified by dividing both the numerator and the denominator by a common factor. In this case, 4 is a common factor of both the numerator and the denominator of 4/16. When we divide both by 4, we get 4/4 = 1 and 16/4 = 4, so the simplified fraction is 1/4. Alternatively, we can think of this in terms of multiplication. For example, if we multiply the numerator and denominator of the fraction 1/4 by 4, we get (1x4)/(4x4), or 4/16. Since both fractions can be derived from the other through multiplication or division by the same number, they represent the same value and are equivalent.', 'input': '4/16', 'instruction': 'Explain why the following fraction is equivalent to 1/4'}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create a Smaller Subset for Testing"
      ],
      "metadata": {
        "id": "uj5S-LivsZ1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_ds = dataset.shuffle(seed=SEED).select(range(int(len(dataset) * 0.01)))\n",
        "print(f\"Subset size: {len(subset_ds)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:49:56.951108Z",
          "iopub.execute_input": "2025-05-17T20:49:56.951359Z",
          "iopub.status.idle": "2025-05-17T20:49:56.961418Z",
          "shell.execute_reply.started": "2025-05-17T20:49:56.951343Z",
          "shell.execute_reply": "2025-05-17T20:49:56.960779Z"
        },
        "id": "VSScM236sZ1K",
        "outputId": "64080a0c-5a75-43d8-b421-ef574f4898fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Subset size: 517\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Format the Data for Instruction Fine-tuning"
      ],
      "metadata": {
        "id": "yQrnpUDqsZ1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the data for instruction fine-tuning\n",
        "def format_instruction(entry):\n",
        "    \"\"\"Format a single entry into an instruction format.\"\"\"\n",
        "    instruction = entry.get('instruction', '')\n",
        "    input_text = entry.get('input', '')\n",
        "\n",
        "    prompt = 'Below is an instruction that describes a task'\n",
        "    if input_text:\n",
        "        prompt += ', paired with an input that provides further context'\n",
        "\n",
        "    prompt += \".\\n\\n\"\n",
        "    prompt += \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    prompt += f\"### Instruction:\\n{instruction}\\n\\n\"\n",
        "\n",
        "    if input_text:\n",
        "        prompt += f\"### Input:\\n{input_text}\\n\\n\"\n",
        "\n",
        "    prompt += \"### Response:\\n\"\n",
        "    return prompt\n",
        "\n",
        "def format_entry(example):\n",
        "    \"\"\"Format an example with prompt and completion.\"\"\"\n",
        "    return {\n",
        "        \"prompt\": format_instruction(example),\n",
        "        \"completion\": example[\"output\"]\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:49:56.962181Z",
          "iopub.execute_input": "2025-05-17T20:49:56.962420Z",
          "iopub.status.idle": "2025-05-17T20:49:56.967541Z",
          "shell.execute_reply.started": "2025-05-17T20:49:56.962396Z",
          "shell.execute_reply": "2025-05-17T20:49:56.966805Z"
        },
        "id": "vMmJB0BFsZ1K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the dataset\n",
        "formatted_dataset = subset_ds.map(lambda x: {\"formatted\": format_entry(x)})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:49:56.969054Z",
          "iopub.execute_input": "2025-05-17T20:49:56.969279Z",
          "iopub.status.idle": "2025-05-17T20:49:56.989741Z",
          "shell.execute_reply.started": "2025-05-17T20:49:56.969257Z",
          "shell.execute_reply": "2025-05-17T20:49:56.989047Z"
        },
        "id": "TSUl_KhdsZ1K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Split into Train and Validation Sets"
      ],
      "metadata": {
        "id": "kHk7dh4dsZ1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and validation sets\n",
        "train_val_dataset = formatted_dataset.train_test_split(test_size=0.01, seed=SEED)\n",
        "train_dataset = train_val_dataset[\"train\"]\n",
        "val_dataset = train_val_dataset[\"test\"]\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:49:56.990450Z",
          "iopub.execute_input": "2025-05-17T20:49:56.990697Z",
          "iopub.status.idle": "2025-05-17T20:49:57.008037Z",
          "shell.execute_reply.started": "2025-05-17T20:49:56.990670Z",
          "shell.execute_reply": "2025-05-17T20:49:57.007363Z"
        },
        "id": "L7oK-pUBsZ1L",
        "outputId": "5f0d188f-7d1d-41ad-d3a8-c31ecc159a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training samples: 511\nValidation samples: 6\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample after formatting\n",
        "sample_formatted = train_dataset[0][\"formatted\"]\n",
        "print(\"\\nSample formatted input:\")\n",
        "print(sample_formatted[\"prompt\"])\n",
        "print(\"\\nSample completion:\")\n",
        "print(sample_formatted[\"completion\"])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:49:57.008864Z",
          "iopub.execute_input": "2025-05-17T20:49:57.009554Z",
          "iopub.status.idle": "2025-05-17T20:49:57.020414Z",
          "shell.execute_reply.started": "2025-05-17T20:49:57.009536Z",
          "shell.execute_reply": "2025-05-17T20:49:57.019708Z"
        },
        "id": "BstIhH9BsZ1M",
        "outputId": "d217c28d-1bb3-416d-b51b-e5cd245cdec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nSample formatted input:\nBelow is an instruction that describes a task.\n\nWrite a response that appropriately completes the request.\n\n### Instruction:\nEdit the following sentence to make it sound more formal: “we spoke on the phone”\n\n### Response:\n\n\nSample completion:\n\"We had a conversation via telephone.\"\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Preparation\n",
        "\n",
        "### 1. Load Tokenizer and Define Tokenization Function"
      ],
      "metadata": {
        "id": "NjKAn5GDsZ1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Choose your model\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B\"  # You can change to \"meta-llama/Llama-2-7b\" if preferred\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # LLaMA uses <eos> as <pad>"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:49:57.021227Z",
          "iopub.execute_input": "2025-05-17T20:49:57.021561Z",
          "iopub.status.idle": "2025-05-17T20:49:59.657805Z",
          "shell.execute_reply.started": "2025-05-17T20:49:57.021537Z",
          "shell.execute_reply": "2025-05-17T20:49:59.657248Z"
        },
        "colab": {
          "referenced_widgets": [
            "9da35e530398429ab53c90df14cc1a81",
            "cc811c347e944fbc929ea01ab3956e70",
            "088090b900d04f1b8fc30c8387a3eac2"
          ]
        },
        "id": "T3X2-zJdsZ1M",
        "outputId": "6fc1b143-db9c-4bd5-d8e3-80aa65c28486"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9da35e530398429ab53c90df14cc1a81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc811c347e944fbc929ea01ab3956e70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "088090b900d04f1b8fc30c8387a3eac2"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize function\n",
        "def tokenize_function(example):\n",
        "    \"\"\"Tokenize the text combining prompt and completion.\"\"\"\n",
        "    formatted = example[\"formatted\"]\n",
        "    full_text = formatted[\"prompt\"] + formatted[\"completion\"]\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        full_text,\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Create labels (same as input_ids but with -100 for prompt tokens)\n",
        "    input_ids = tokenized[\"input_ids\"][0]\n",
        "\n",
        "    # Clone for labels\n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    # Find the position where the completion starts\n",
        "    prompt_ids = tokenizer(formatted[\"prompt\"], return_tensors=\"pt\")[\"input_ids\"][0]\n",
        "    prompt_length = len(prompt_ids)\n",
        "\n",
        "    # Set labels to -100 for prompt tokens (we don't want to compute loss on these)\n",
        "    labels[:prompt_length] = -100\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"][0],\n",
        "        \"labels\": labels\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:49:59.658512Z",
          "iopub.execute_input": "2025-05-17T20:49:59.658697Z",
          "iopub.status.idle": "2025-05-17T20:49:59.664197Z",
          "shell.execute_reply.started": "2025-05-17T20:49:59.658683Z",
          "shell.execute_reply": "2025-05-17T20:49:59.663563Z"
        },
        "id": "vhUyP157sZ1M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Tokenize Datasets"
      ],
      "metadata": {
        "id": "Q2ADHLXZsZ1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize datasets\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "tokenized_val_dataset = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"Tokenized train dataset: {len(tokenized_train_dataset)}\")\n",
        "print(f\"Tokenized validation dataset: {len(tokenized_val_dataset)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:49:59.664796Z",
          "iopub.execute_input": "2025-05-17T20:49:59.665370Z",
          "iopub.status.idle": "2025-05-17T20:50:01.308947Z",
          "shell.execute_reply.started": "2025-05-17T20:49:59.665347Z",
          "shell.execute_reply": "2025-05-17T20:50:01.308087Z"
        },
        "colab": {
          "referenced_widgets": [
            "59df366343d043b29ed077887160edc2",
            "2531c8257e8147738f074f0d1b4bc5cf"
          ]
        },
        "id": "Pt0IEJGrsZ1O",
        "outputId": "52ed8cef-670c-440e-e5cc-f6936753eff6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/511 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59df366343d043b29ed077887160edc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/6 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2531c8257e8147738f074f0d1b4bc5cf"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Tokenized train dataset: 511\nTokenized validation dataset: 6\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and QLoRA Configuration\n",
        "\n",
        "### 1. Define QLoRA Parameters and BitsAndBytes Configuration"
      ],
      "metadata": {
        "id": "l-BTUezdsZ1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define QLoRA parameters\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "\n",
        "# QLoRA parameters\n",
        "lora_r = 8\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "\n",
        "# BitsAndBytes parameters\n",
        "use_4bit = True\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# BitsAndBytes configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:50:01.312224Z",
          "iopub.execute_input": "2025-05-17T20:50:01.312448Z",
          "iopub.status.idle": "2025-05-17T20:50:15.001729Z",
          "shell.execute_reply.started": "2025-05-17T20:50:01.312432Z",
          "shell.execute_reply": "2025-05-17T20:50:15.001115Z"
        },
        "id": "f_-mCu6xsZ1P",
        "outputId": "f45e5874-ccf8-4c48-c3fc-b74b74550345"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-05-17 20:50:03.361907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747515003.562335      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747515003.614226      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load Base Model with Quantization"
      ],
      "metadata": {
        "id": "zhO1AuH6sZ1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load base model with quantization\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=device_map,\n",
        "    token=True  # Set to True if the model requires authentication\n",
        ")\n",
        "\n",
        "# Prepare model for training\n",
        "model.config.use_cache = False  # Disable KV cache for training\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:50:15.002462Z",
          "iopub.execute_input": "2025-05-17T20:50:15.002937Z",
          "iopub.status.idle": "2025-05-17T20:52:14.815715Z",
          "shell.execute_reply.started": "2025-05-17T20:50:15.002920Z",
          "shell.execute_reply": "2025-05-17T20:52:14.815163Z"
        },
        "colab": {
          "referenced_widgets": [
            "35e89ed4b853465384ca39c46a1048ff",
            "02cbdb54b8204d448e4cf0427ea8d869",
            "88fbae58bcd44a85a12bfd96ebf23f6a",
            "93253354cb464bcfb272d7bcdc3ceeb7",
            "bd2891a5d95949e7a5d038961bb9eab0",
            "cc867618369d4b98b84aa6706f5936b5",
            "8ba40a861b4146babeb704fa6bd4bdf3",
            "a594163b8f874ec7b08e99ec0533243c",
            "50224d4d28ed41a39f576d914ca873a2"
          ]
        },
        "id": "NaB7ZQMXsZ1P",
        "outputId": "291fc10d-47c5-4847-eafe-9b14b7292870"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35e89ed4b853465384ca39c46a1048ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02cbdb54b8204d448e4cf0427ea8d869"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88fbae58bcd44a85a12bfd96ebf23f6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93253354cb464bcfb272d7bcdc3ceeb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd2891a5d95949e7a5d038961bb9eab0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc867618369d4b98b84aa6706f5936b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ba40a861b4146babeb704fa6bd4bdf3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a594163b8f874ec7b08e99ec0533243c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50224d4d28ed41a39f576d914ca873a2"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Configure LoRA"
      ],
      "metadata": {
        "id": "jHSrg_OSsZ1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA configuration\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        # \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        # \"o_proj\",\n",
        "        # \"gate_proj\",\n",
        "        # \"up_proj\",\n",
        "        # \"down_proj\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:52:14.816455Z",
          "iopub.execute_input": "2025-05-17T20:52:14.816659Z",
          "iopub.status.idle": "2025-05-17T20:52:14.821211Z",
          "shell.execute_reply.started": "2025-05-17T20:52:14.816643Z",
          "shell.execute_reply": "2025-05-17T20:52:14.820506Z"
        },
        "id": "-CpGBViLsZ1Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup and Execution\n",
        "\n",
        "### 1. Configure Training Arguments"
      ],
      "metadata": {
        "id": "rpG_bZxDsZ1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure training arguments\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "output_dir = \"./llama3-8b-finetuned\"\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,  # effective batch size = 4\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=25,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    bf16=False,  # Set to True if using A100 or H100\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_dir=\"./logs\",\n",
        "    seed=SEED,\n",
        "    data_seed=SEED,\n",
        "    push_to_hub=False,  # Set to True if you want to upload to HF Hub\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:52:14.821969Z",
          "iopub.execute_input": "2025-05-17T20:52:14.822203Z",
          "iopub.status.idle": "2025-05-17T20:52:14.909563Z",
          "shell.execute_reply.started": "2025-05-17T20:52:14.822182Z",
          "shell.execute_reply": "2025-05-17T20:52:14.908970Z"
        },
        "id": "mqdBb_h5sZ1Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Initialize SFT Trainer and Start Training"
      ],
      "metadata": {
        "id": "2bhdbt8IsZ1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SFT Trainer\n",
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T20:52:14.910240Z",
          "iopub.execute_input": "2025-05-17T20:52:14.910463Z",
          "iopub.status.idle": "2025-05-17T21:58:40.212679Z",
          "shell.execute_reply.started": "2025-05-17T20:52:14.910447Z",
          "shell.execute_reply": "2025-05-17T21:58:40.212019Z"
        },
        "colab": {
          "referenced_widgets": [
            "5e635c4de8b34897a9316ecb6854d22e",
            "70cd4d2b903f446c93882891236ff617"
          ]
        },
        "id": "m8Ogt7VusZ1R",
        "outputId": "25076227-6fa8-4bb4-8557-a3b735139997"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Truncating train dataset:   0%|          | 0/511 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e635c4de8b34897a9316ecb6854d22e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Truncating eval dataset:   0%|          | 0/6 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70cd4d2b903f446c93882891236ff617"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Starting training...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [127/127 1:05:46, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>4.886800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.215700</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.190300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.186400</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.185800</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=127, training_loss=1.1180327210839338, metrics={'train_runtime': 3978.9847, 'train_samples_per_second': 0.128, 'train_steps_per_second': 0.032, 'total_flos': 2.343464713637069e+16, 'train_loss': 1.1180327210839338})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation and Saving\n",
        "\n",
        "### 1. Evaluate the Model"
      ],
      "metadata": {
        "id": "UNnxHiygsZ1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation set\n",
        "print(\"Evaluating model...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T21:58:40.213541Z",
          "iopub.execute_input": "2025-05-17T21:58:40.213810Z",
          "iopub.status.idle": "2025-05-17T21:58:56.261848Z",
          "shell.execute_reply.started": "2025-05-17T21:58:40.213792Z",
          "shell.execute_reply": "2025-05-17T21:58:56.261149Z"
        },
        "id": "Wg4pokgmsZ1S",
        "outputId": "4c658651-3126-4911-f6a5-5f4f57bbdc9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Evaluating model...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 00:13]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Evaluation results: {'eval_loss': 0.2131618708372116, 'eval_runtime': 16.0364, 'eval_samples_per_second': 0.374, 'eval_steps_per_second': 0.374}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Save the Fine-tuned Model"
      ],
      "metadata": {
        "id": "Eh8z2oNJsZ1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Model saved to {output_dir}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T21:58:56.262647Z",
          "iopub.execute_input": "2025-05-17T21:58:56.262912Z",
          "iopub.status.idle": "2025-05-17T21:58:57.026974Z",
          "shell.execute_reply.started": "2025-05-17T21:58:56.262889Z",
          "shell.execute_reply": "2025-05-17T21:58:57.026148Z"
        },
        "id": "Ks9xG6masZ1S",
        "outputId": "13927f7b-8651-4774-c3e5-b2ab45073b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Model saved to ./llama3-8b-finetuned\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to Hugging Face Hub"
      ],
      "metadata": {
        "id": "g0Wo8DrAsZ1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up your model card information\n",
        "model_name = \"llama3-8b-finetuned-alpaca\"\n",
        "repo_name = f\"aashu-0/{model_name}\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T21:58:57.027832Z",
          "iopub.execute_input": "2025-05-17T21:58:57.028059Z",
          "iopub.status.idle": "2025-05-17T21:58:57.031762Z",
          "shell.execute_reply.started": "2025-05-17T21:58:57.028041Z",
          "shell.execute_reply": "2025-05-17T21:58:57.031001Z"
        },
        "id": "piXcSuMZsZ1V"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Push to hub\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Configure model card with training information\n",
        "model_card = f\"\"\"\n",
        "# {model_name}\n",
        "\n",
        "This model is a fine-tuned version of [{model_id}](https://huggingface.co/{model_id}) on the Alpaca dataset.\n",
        "\n",
        "## Training Parameters\n",
        "- **Training Dataset**: Alpaca\n",
        "- **Dataset Size**: {len(train_dataset)} samples\n",
        "- **QLoRA Parameters**: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}\n",
        "- **Learning Rate**: {training_arguments.learning_rate}\n",
        "- **Batch Size**: {training_arguments.per_device_train_batch_size * training_arguments.gradient_accumulation_steps}\n",
        "- **Epochs**: {training_arguments.num_train_epochs}\n",
        "\n",
        "## Evaluation Results\n",
        "{eval_results}\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T21:58:57.032609Z",
          "iopub.execute_input": "2025-05-17T21:58:57.032858Z",
          "iopub.status.idle": "2025-05-17T21:58:57.048959Z",
          "shell.execute_reply.started": "2025-05-17T21:58:57.032836Z",
          "shell.execute_reply": "2025-05-17T21:58:57.048340Z"
        },
        "id": "I_aDBqP3sZ1W"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model card\n",
        "with open(f\"{output_dir}/README.md\", \"w\") as f:\n",
        "    f.write(model_card)\n",
        "\n",
        "trainer.push_to_hub()\n",
        "\n",
        "print(f\"Model pushed to Hugging Face Hub: {repo_name}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:01:16.089690Z",
          "iopub.execute_input": "2025-05-17T22:01:16.090435Z",
          "iopub.status.idle": "2025-05-17T22:01:23.262048Z",
          "shell.execute_reply.started": "2025-05-17T22:01:16.090409Z",
          "shell.execute_reply": "2025-05-17T22:01:23.261264Z"
        },
        "colab": {
          "referenced_widgets": [
            "b1e5b7c1746243b1b9cd72240861a459"
          ]
        },
        "id": "3ZlUeq_ysZ1W",
        "outputId": "64d1e8be-096e-48de-bbba-e30135db5dd0"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Repo card metadata block was not found. Setting CardData to empty.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1e5b7c1746243b1b9cd72240861a459"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Model pushed to Hugging Face Hub: aashu-0/llama3-8b-finetuned-alpaca\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Example"
      ],
      "metadata": {
        "id": "fikCO7CbsZ1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model for inference\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load base model\n",
        "model_base = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map={\"\": 0},\n",
        "    token=True\n",
        ")\n",
        "\n",
        "# Load LoRA configuration and model\n",
        "peft_model_id = output_dir\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = PeftModel.from_pretrained(model_base, peft_model_id)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Create a text generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:02:44.695566Z",
          "iopub.execute_input": "2025-05-17T22:02:44.696432Z",
          "iopub.status.idle": "2025-05-17T22:03:28.053080Z",
          "shell.execute_reply.started": "2025-05-17T22:02:44.696401Z",
          "shell.execute_reply": "2025-05-17T22:03:28.052506Z"
        },
        "colab": {
          "referenced_widgets": [
            "f7d855f127c84c10ab0e6e56e0639d12"
          ]
        },
        "id": "cHmSkLCFsZ1W",
        "outputId": "55a834dc-7a21-42a8-c4b3-3871366f263f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7d855f127c84c10ab0e6e56e0639d12"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\nThe model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with a few prompts\n",
        "test_prompts = [\n",
        "    \"Write about artificial intelligence.\",\n",
        "    \"Explain quantum computing to a 10-year-old.\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    formatted_prompt = format_instruction({\"instruction\": prompt})\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    result = pipe(formatted_prompt)[0]['generated_text']\n",
        "    response = result.split(\"### Response:\")[1].strip()\n",
        "    print(f\"Response:\\n{response}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:03:46.417345Z",
          "iopub.execute_input": "2025-05-17T22:03:46.417612Z",
          "iopub.status.idle": "2025-05-17T22:04:08.249017Z",
          "shell.execute_reply.started": "2025-05-17T22:03:46.417595Z",
          "shell.execute_reply": "2025-05-17T22:04:08.248366Z"
        },
        "id": "AJXIg1XEsZ1X",
        "outputId": "8c4d1470-751b-4fc5-9f25-f811cce6089c"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nPrompt: Write about artificial intelligence.\nResponse:\nArtificial Intelligence (AI) refers to the development of computer systems capable of performing tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and learning. AI has become increasingly prevalent in recent years, with applications ranging from self-driving cars to virtual assistants like Siri and Alexa. While there are concerns surrounding the potential negative impact of AI on society, it also offers many benefits, including improved efficiency, enhanced productivity, and increased personalization. Ultimately, Artificial Intelligence holds great promise for the future, but its proper implementation and regulation must be prioritized to ensure responsible use and ethical outcomes.\n\nPrompt: Explain quantum computing to a 10-year-old.\nResponse:\nQuantum Computing is like using magical powers from science fiction movies. It's super cool and amazing! Quantum Computers use tiny particles called \"quanta\" (the plural of quantum). These quanta are very small, but they can have special properties like being able to exist in multiple places at once or be connected through what we call \"entanglement.\"\n\nThe power comes when you put lots of these quanta together. Imagine having billions of quanta all working together as one huge machine. That's how powerful it would be!\n\nInstead of just thinking about things in either black or white, yes or no, on or off...quantum computers allow us to think about them in many different ways at once. We could even find solutions to problems too complex for normal computers. \n\nIt might sound impossible right now, but someday soon people will be making new discoveries with this amazing technology!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}